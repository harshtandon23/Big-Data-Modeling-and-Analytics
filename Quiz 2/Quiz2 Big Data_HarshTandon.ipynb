{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Quiz 2\n",
    "### Author: Harsh Tandon W1580393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Spark and start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text file containing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a Fox jumped high and high and jumped and jumped',\n",
       " 'fox of red jumped',\n",
       " 'fox of blue jumped',\n",
       " 'a Fox is a red fox of hen ',\n",
       " 'a fox is a high fox',\n",
       " 'orange fox is high and blue and blue']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = 'D:/3rd Qtr Study Material/Big Data Modeling/Quiz/foxy2.txt'\n",
    "recs = sc.textFile(input_path)\n",
    "recs.collect() #showing records read from txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract single words by using flatMap(). We'll get the words by splitting on whitespace, and words will be converted to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split(\" \")\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = recs.flatMap(Func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Words:\n",
      " ['a', 'fox', 'jumped', 'high', 'and', 'high', 'and', 'jumped', 'and', 'jumped', 'fox', 'of', 'red', 'jumped', 'fox', 'of', 'blue', 'jumped', 'a', 'fox', 'is', 'a', 'red', 'fox', 'of', 'hen', '', 'a', 'fox', 'is', 'a', 'high', 'fox', 'orange', 'fox', 'is', 'high', 'and', 'blue', 'and', 'blue']\n"
     ]
    }
   ],
   "source": [
    "#words = recs.flatMap(lambda r: r.lower().split(\" \"))\n",
    "print(\"Single Words:\\n\", words.collect()) #showing the single words extracted from records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the extracted words to remove words whose length is less than 3. We can see in the filter's output that no word's length is less than 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words:\n",
      " ['fox', 'jumped', 'high', 'and', 'high', 'and', 'jumped', 'and', 'jumped', 'fox', 'red', 'jumped', 'fox', 'blue', 'jumped', 'fox', 'red', 'fox', 'hen', 'fox', 'high', 'fox', 'orange', 'fox', 'high', 'and', 'blue', 'and', 'blue']\n"
     ]
    }
   ],
   "source": [
    "filtered = words.filter(lambda x: len(x) >= 3)\n",
    "print(\"Filtered Words:\\n\", filtered.collect()) #showing output after filtering is done\n",
    "\n",
    "\n",
    "#filter out stop words\n",
    "#stopWords = ['and','is','the','he','she','am','be','I']\n",
    "#filtered = words.filter(lambda x: x not in stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pairs using the map function to assign count to each filtered word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapper's Output:\n",
      " [('fox', 1), ('jumped', 1), ('high', 1), ('and', 1), ('high', 1), ('and', 1), ('jumped', 1), ('and', 1), ('jumped', 1), ('fox', 1), ('red', 1), ('jumped', 1), ('fox', 1), ('blue', 1), ('jumped', 1), ('fox', 1), ('red', 1), ('fox', 1), ('hen', 1), ('fox', 1), ('high', 1), ('fox', 1), ('orange', 1), ('fox', 1), ('high', 1), ('and', 1), ('blue', 1), ('and', 1), ('blue', 1)]\n"
     ]
    }
   ],
   "source": [
    "pairs = filtered.map(lambda x : (x, 1))\n",
    "print(\"Mapper's Output:\\n\", pairs.collect()) #showing generated pairs for each word\n",
    "\n",
    "\n",
    "#group the data according to first 3 characters of any element\n",
    "#rdd4 = filtered.groupBy(lambda w: w[0:3])\n",
    "#print [(k,list(v)) for (k,v) in rdd4.take(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word frequency by using reduceByKey(). The output shows count of each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducer's Output:\n",
      " [('high', 4), ('hen', 1), ('orange', 1), ('fox', 8), ('jumped', 5), ('and', 5), ('red', 2), ('blue', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_freq = pairs.reduceByKey(lambda x, y: x+y)\n",
    "print(\"Reducer's Output:\\n\", word_freq.collect()) #showing output of reduceByKey function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the output of reducer to show only those words which appear at least 3 times. We'll use a filter() to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final required Output:\n",
      " [('high', 4), ('fox', 8), ('jumped', 5), ('and', 5), ('blue', 3)]\n"
     ]
    }
   ],
   "source": [
    "word_freq_gt3 = word_freq.filter(lambda x: x[1]>=3)\n",
    "print(\"Final required Output:\\n\", word_freq_gt3.collect()) #showing the required output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find sum of first 10,000 numbers\n",
    "num_rdd = sc.parallelize(range(1,10000))\n",
    "num_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() #stop spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
